10/18/2024 16:21:59 - INFO - __main__ - Distributed environment: MULTI_CPU  Backend: gloo
Num processes: 2
Process index: 0
Local process index: 0
Device: cpu:0

Mixed precision type: bf16

10/18/2024 16:21:59 - INFO - __main__ - Distributed environment: MULTI_CPU  Backend: gloo
Num processes: 2
Process index: 1
Local process index: 1
Device: cpu:0

Mixed precision type: bf16

You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 932.59it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 884.69it/s]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:28<00:28, 28.42s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:28<00:28, 28.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 27.24s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 27.42s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 27.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 27.48s/it]
{'use_learned_positional_embeddings'} was not found in config. Values will be initialized to default values.
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded scheduler as CogVideoXDDIMScheduler from `scheduler` subfolder of THUDM/CogVideoX-2b.
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded tokenizer as T5Tokenizer from `tokenizer` subfolder of THUDM/CogVideoX-2b.
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 25.86it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 32.26it/s]
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 28.61it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 35.68it/s]
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded scheduler as CogVideoXDDIMScheduler from `scheduler` subfolder of THUDM/CogVideoX-2b.
Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]Loaded tokenizer as T5Tokenizer from `tokenizer` subfolder of THUDM/CogVideoX-2b.
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 29.90it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 37.31it/s]
Loading pipeline components...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 29.90it/s]Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 37.28it/s]
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: lyricmayday (yangluo). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.3
wandb: Run data is saved locally in /home/svu/e0833524/CogVideo/finetune/wandb/run-20241018_162420-d0apsqjj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-plant-6
wandb: â­ï¸ View project at https://wandb.ai/yangluo/cogvideox-lora
wandb: ðŸš€ View run at https://wandb.ai/yangluo/cogvideox-lora/runs/d0apsqjj
10/18/2024 16:24:22 - INFO - __main__ - ***** Running training *****
10/18/2024 16:24:22 - INFO - __main__ -   Num trainable parameters = 1700926302
10/18/2024 16:24:22 - INFO - __main__ -   Num examples = 69
10/18/2024 16:24:22 - INFO - __main__ -   Num batches each epoch = 35
10/18/2024 16:24:22 - INFO - __main__ -   Num epochs = 30
10/18/2024 16:24:22 - INFO - __main__ -   Instantaneous batch size per device = 1
10/18/2024 16:24:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2
10/18/2024 16:24:22 - INFO - __main__ -   Gradient accumulation steps = 1
10/18/2024 16:24:22 - INFO - __main__ -   Total optimization steps = 1050
10/18/2024 16:24:22 - INFO - __main__ -   KD running correctly
Steps:   0%|          | 0/1050 [00:00<?, ?it/s]
  0%|          | 0/50 [00:00<?, ?it/s][A  0%|          | 0/50 [00:00<?, ?it/s]
  2%|â–         | 1/50 [17:49<14:33:33, 1069.66s/it][A  2%|â–         | 1/50 [18:13<14:53:14, 1093.76s/it]
  4%|â–         | 2/50 [35:37<14:14:53, 1068.61s/it][A  4%|â–         | 2/50 [36:24<14:33:43, 1092.15s/it]
  6%|â–Œ         | 3/50 [53:24<13:56:24, 1067.76s/it][A  6%|â–Œ         | 3/50 [54:34<14:14:43, 1091.14s/it]
  8%|â–Š         | 4/50 [1:11:11<13:38:28, 1067.58s/it][A  8%|â–Š         | 4/50 [1:12:44<13:56:13, 1090.72s/it]
 10%|â–ˆ         | 5/50 [1:28:58<13:20:35, 1067.45s/it][A 10%|â–ˆ         | 5/50 [1:30:54<13:37:51, 1090.49s/it]
 12%|â–ˆâ–        | 6/50 [1:46:46<13:02:48, 1067.47s/it][A 12%|â–ˆâ–        | 6/50 [1:49:04<13:19:31, 1090.26s/it]
 14%|â–ˆâ–        | 7/50 [2:04:34<12:45:09, 1067.67s/it][A 14%|â–ˆâ–        | 7/50 [2:07:14<13:01:18, 1090.20s/it]
 16%|â–ˆâ–Œ        | 8/50 [2:22:24<12:27:53, 1068.41s/it][A 16%|â–ˆâ–Œ        | 8/50 [2:25:25<12:43:20, 1090.50s/it]
 18%|â–ˆâ–Š        | 9/50 [2:40:13<12:10:10, 1068.55s/it][A 18%|â–ˆâ–Š        | 9/50 [2:43:36<12:25:06, 1090.40s/it]
 20%|â–ˆâ–ˆ        | 10/50 [2:58:02<11:52:28, 1068.71s/it][A 20%|â–ˆâ–ˆ        | 10/50 [3:01:46<12:06:53, 1090.34s/it]
 22%|â–ˆâ–ˆâ–       | 11/50 [3:15:52<11:34:56, 1069.13s/it][A 22%|â–ˆâ–ˆâ–       | 11/50 [3:19:56<11:48:40, 1090.27s/it]
 24%|â–ˆâ–ˆâ–       | 12/50 [3:33:42<11:17:23, 1069.55s/it][A 24%|â–ˆâ–ˆâ–       | 12/50 [3:38:06<11:30:27, 1090.21s/it]
 26%|â–ˆâ–ˆâ–Œ       | 13/50 [3:51:32<10:59:38, 1069.70s/it][A 26%|â–ˆâ–ˆâ–Œ       | 13/50 [3:56:16<11:12:17, 1090.21s/it]
 28%|â–ˆâ–ˆâ–Š       | 14/50 [4:09:21<10:41:41, 1069.48s/it][A 28%|â–ˆâ–ˆâ–Š       | 14/50 [4:14:26<10:54:06, 1090.19s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [4:27:10<10:23:45, 1069.31s/it][A 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [4:32:37<10:35:56, 1090.19s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [4:44:59<10:05:51, 1069.17s/it][A 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [4:50:47<10:17:45, 1090.17s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [5:02:52<9:48:36, 1070.20s/it] [A 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [5:09:19<10:03:19, 1096.96s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [5:21:25<9:37:43, 1083.25s/it][A 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [5:27:56<9:48:15, 1102.97s/it] 
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [5:39:48<9:22:40, 1089.03s/it][A 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [5:46:26<9:30:56, 1105.06s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [5:57:39<9:01:53, 1083.78s/it][A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [6:04:34<9:10:00, 1100.00s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [6:15:32<8:42:10, 1080.35s/it][A 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [6:22:43<8:49:59, 1096.52s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [6:33:22<8:22:45, 1077.34s/it][A 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [6:40:53<8:30:48, 1094.59s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [6:51:12<8:03:50, 1075.22s/it][A 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [6:59:03<8:11:59, 1093.31s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [7:09:01<7:45:06, 1073.34s/it][A 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [7:17:14<7:53:22, 1092.41s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [7:26:50<7:26:39, 1071.96s/it][A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [7:35:24<7:34:54, 1091.78s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [7:44:37<7:08:11, 1070.48s/it][A 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [7:53:34<7:16:30, 1091.27s/it]